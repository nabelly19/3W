{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3W Dataset's General Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a general presentation of the 3W Dataset, to the best of its authors' knowledge, the first realistic and public dataset with rare undesirable real events in oil wells that can be readily used as a benchmark dataset for development of machine learning techniques related to inherent difficulties of actual data.\n",
    "\n",
    "For more information about the theory behind this dataset, refer to the paper **A Realistic and Public Dataset with Rare Undesirable Real Events in Oil Wells** published in the **Journal of Petroleum Science and Engineering** (link [here](https://doi.org/10.1016/j.petrol.2019.106223))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook presents the 3W Dataset 2.0.0 in a general way. For this, some functionalities for data unification and the benefits of this process are demonstrated.\n",
    "\n",
    "In complex datasets like 3W, data is often distributed across multiple folders and files, which may hinder quick insights and analysis. The data unification process involves loading, cleaning, and merging these scattered files into a single, well-structured data frame. This process offers several key benefits:\n",
    "\n",
    "Functionalities of Data Unification\n",
    "Automated Loading of Distributed Data:\n",
    "\n",
    "The notebook loads all Parquet files from multiple folders efficiently.\n",
    "It filters out irrelevant files (e.g., simulated data) and extracts important metadata like timestamps directly from file names.\n",
    "Data Normalization:\n",
    "\n",
    "Additional columns (e.g., folder ID, date, and time) are added, ensuring consistency across different data points.\n",
    "This enhances downstream analysis by making sure that different segments are harmonized.\n",
    "Handling Large-Scale Data with Dask:\n",
    "\n",
    "The use of Dask allows seamless processing of large datasets that would otherwise not fit into memory.\n",
    "This makes it easier to explore and manipulate the entire dataset efficiently.\n",
    "Benefits of Data Unification\n",
    "Improved Data Accessibility:\n",
    "With all data combined into a single structure, researchers and engineers can access relevant information faster, minimizing the time spent searching across files.\n",
    "\n",
    "Enhanced Analytical Capabilities:\n",
    "Unified data allows for richer analytics, such as visualizing trends and patterns across the entire dataset. Anomalies and transient events can be identified and classified more accurately.\n",
    "\n",
    "Simplified Visualization:\n",
    "By consolidating data into a single DataFrame, it's easier to generate comprehensive visualizations that provide meaningful insights about operational states.\n",
    "\n",
    "Facilitates Collaboration:\n",
    "When datasets are standardized and merged, it becomes easier for teams to share their findings and collaborate on data-driven projects. The unified dataset serves as a single source of truth.\n",
    "\n",
    "This notebook demonstrates these functionalities and benefits by loading the 3W Dataset, classifying events across multiple operational states, and generating visualizations that offer a deeper understanding of system behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from toolkit.misc import load_and_combine_data, classify_events, visualize_data\n",
    "\n",
    "plt.style.use('ggplot')  # Estilo do matplotlib\n",
    "pd.set_option('display.max_columns', None)  # Exibe todas as colunas do DataFrame\n",
    "\n",
    "dataset_dir = \"C:/Users/anabe/OneDrive/Área de Trabalho/HACKATHON PETROBRÁS/dataset_modificado/dataset_modificado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Instances' Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explain the organization of the folders and files within the dataset. The 3W Dataset contains subfolders numbered from 0 to 9, where each folder represents a specific operational situation, as described below:\n",
    "\n",
    "* 0 = Normal Operation\n",
    "* 1 = Abrupt Increase of BSW\n",
    "* 2 = Spurious Closure of DHSV\n",
    "* 3 = Severe Slugging\n",
    "* 4 = Flow Instability\n",
    "* 5 = Rapid Productivity Loss\n",
    "* 6 = Quick Restriction in PCK\n",
    "* 7 = Scaling in PCK\n",
    "* 8 = Hydrate in Production Line\n",
    "* 9 = Hydrate in Service Line\n",
    "\n",
    "Each file follows the naming pattern:\n",
    "* WELL-00008_20170818000222.parquet\n",
    "\n",
    "* WELL-00008: Identification of the well.\n",
    "* 20170818000222: Timestamp in the format yyyyMMddhhmmss.\n",
    "* .parquet: File extension indicating the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolkit.misc import load_and_combine_data, classify_events, visualize_data\n",
    "\n",
    "datatype = 'SIMULATED'\n",
    "df = load_and_combine_data(dataset_dir, datatype)\n",
    "\n",
    "if df is not None:\n",
    "    event_summary = classify_events(df)\n",
    "\n",
    "    visualize_data(event_summary)\n",
    "else:\n",
    "    print(\"Nenhum dado foi carregado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
